arch:
  type: TransformerLMHeadModel
  args:
    transformer_config:
      type: TransformerDecoderOnlyModel
      args:
        embed_config:
          type: TransformerEmbeddingBlock
          args:
            token_embed_config:
              type: TokenEmbedding
            p_drop_embed: 0.0
        decoder_config:
          type: TransformerDecoderBlock
          args:
            attn_config:
              type: LlamaAttention
              args:
                p_drop_attn: 0.0
                p_drop_resid: 0.0
                bias_attn: True
                bias_proj: False
                cross_attn: False
                scale_dot_product: True
                scale_layer_wise: False
                rope_config:
                  type: MistralRotaryEmbedding
                  args:
                    base: 1.0e06
            mlp_config:
              type: LlamaMLP
              args:
                n_embed: ~
                n_inner: ~
                act_fn_config:
                  type: SiLUActivation
            ln_config:
              type: LlamaRMSNorm
        ln_config:
          type: LlamaRMSNorm
    lm_head_config:
      type: TransformerLMHead
      args:
        do_transform: False

models:
  Qwen-1.5-0.5B:
    n_embed: 1024
    n_layer: 24
    n_head: 16
    n_key_value_head: 16
    head_size: 64
    n_inner: 2816
    rotary_head_size: 64

global_params:
  n_vocab: 151936
  n_pos: 32768
  ln_eps: 1.0e-06

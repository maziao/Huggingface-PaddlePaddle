arch:
  type: GPT2Config
  args:
    embed_config:
      type: TransformerEmbeddingConfig
      args:
        n_embed: 768
        n_vocab: 50257
        n_pos: 1024
        p_drop_embed: 0.1
    decoder_config:
      type: TransformerDecoderBlockConfig
      args:
        attn_config:
          type: AttentionConfig
          args:
            n_embed: 768
            n_pos: 1024
            n_head: 12
            head_size: 64
            act_fn: relu
            p_drop_attn: 0.1
            p_drop_resid: 0.1
            bias_attn: True
            bias_proj: True
            cross_attn: False
            scale_dot_product: True
            scale_layer_wise: False
        mlp_config:
          type: MLPConfig
          args:
            n_embed: 768
            n_inner: 3072
            act_fn: gelu_new
            p_drop_mlp: 0.1
        n_embed: 768
        ln_eps: 1.0e-5
    n_embed: 768
    n_layer: 12
    ln_eps: 1.0e-5

tasks:
  generation:
    type: TransformerLMHeadModel
  classification:
    type: TransformerClassificationHeadModel
  summary:
    type: TransformerSequenceSummaryHeadModel

scales:
  gpt2-tiny:
    n_embed: 128
    n_layer: 2
    n_head: 2
    head_size: 64
    n_inner: 512
  gpt2:
    n_embed: 768
    n_layer: 12
    n_head: 12
    head_size: 64
    n_inner: 3072
  gpt2-medium:
    n_embed: 1024
    n_layer: 24
    n_head: 16
    head_size: 64
    n_inner: 4096
  gpt2-large:
    n_embed: 1280
    n_layer: 36
    n_head: 20
    head_size: 64
    n_inner: 5120
  gpt2-xl:
    n_embed: 1600
    n_layer: 48
    n_head: 25
    head_size: 64
    n_inner: 6400

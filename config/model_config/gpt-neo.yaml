arch:
  type: TransformerLMHeadModel
  args:
    transformer_config:
      type: TransformerDecoderOnlyModel
      args:
        embed_config:
          type: TransformerEmbedding
          args:
            p_drop_embed: 0.0
        decoder_config:
          type: TransformerDecoderBlock
          args:
            attn_config:
              type: MultiHeadKeyValueAttention
              args:
                p_drop_attn: 0.0
                p_drop_resid: 0.0
                bias_attn: False
                bias_proj: True
                cross_attn: False
                scale_dot_product: False
                scale_layer_wise: False
            mlp_config:
              type: TransformerMLP
              args:
                act_fn:
                  type: NewGELUActivation
                  args: {}
                p_drop_mlp: 0.0
        attn_window_size_loop_unit: [-1, 256]
    lm_head_config:
      type: TransformerLMHead
      args:
        do_transform: False

models:
  gpt-neo-125m:
    n_embed: 768
    n_layer: 12
    n_head: 12
    head_size: 64
    n_inner: 3072

global_params:
  n_vocab: 50257
  n_pos: 2048
  ln_eps: 1.0e-5

stopping_criteria:
  max_length: 20
  max_new_tokens: ~
  min_length: 0
  min_new_tokens: ~
  early_stopping: False
  max_time: ~

# Parameters that control the generation strategy used
do_sample: False
num_beams: 1
num_beam_groups: 1
penalty_alpha: ~
use_cache: True

# Parameters for manipulation of the model output logits
temperature: 1.0
top_k: 50
top_p: 1.0
typical_p: 1.0
epsilon_cutoff: 0.0
eta_cutoff: 0.0
diversity_penalty: 0.0
repetition_penalty: 1.0
encoder_repetition_penalty: 1.0
length_penalty: 1.0
no_repeat_ngram_size: 0
bad_words_ids: ~
force_words_ids: ~
renormalize_logits: False
constraints: ~
forced_bos_token_id: ~
forced_eos_token_id: ~
remove_invalid_values: False
exponential_decay_length_penalty: ~
suppress_tokens: ~
begin_suppress_tokens: ~
forced_decoder_ids: ~

# Parameters that define the output variables of `generate`
num_return_sequences: 1
output_attentions: False
output_hidden_states: False
output_scores: False
return_dict_in_generate: False

# Special tokens that can be used at generation time
pad_token_id: ~
bos_token_id: ~
eos_token_id: ~

# Generation parameters exclusive to encoder-decoder models
encoder_no_repeat_ngram_size: 0
decoder_start_token_id: ~


logit_processor:
  -
    type: HammingDiversityLogitProcessor
    args:
      diversity_penalty: ~
      num_beams: ~
      num_beam_groups: ~
  -
    type: EncoderRepetitionPenaltyLogitProcessor
    args:
      penalty: ~
      encoder_input_ids: ~


stop_criteria:
  -
    type: MaxLengthCriteria
    max_length: 100
  -
    type: MaxTimeCriteria,
    max_time: ~
